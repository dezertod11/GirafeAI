{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqEpGyyyGE1Z",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "## Домашнее задание №3\n",
    "\n",
    "В данном задании вам необходимо реализовать функции ошибки для линейной регрессии и их производные по параметрам, __не используя автоматические дифференцирование.__ Все методы должны быть реализованы только с использованием библиотеки `numpy`. \n",
    "\n",
    "Ваша основная задача: вывести формулы для производных __MSE, MAE, L1 и L2 регуляризационных членов__ в _векторном случае_ (т.е. когда и объект $\\mathbf{x}_i$, и целевое значение $\\mathbf{y}_i$ являются векторами.\n",
    "\n",
    "\n",
    "Для работы вновь обратимся к [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Он был предобработан для вашего удобства и будет загружен ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy pandas seaborn scikit-learn matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you are using Google Colab, uncomment the next line to download `boston_subset.json`\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you are using Google Colab, uncomment the next line to download `boston_subset.json`\n",
    "\"\"\"\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/girafe-ai/ml-course/refs/heads/24f_mipt/homeworks/assignment03_derivatives/boston_subset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lQUR89nGE1f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGf3ShTNGE1q"
   },
   "outputs": [],
   "source": [
    "with open(\"boston_subset.json\") as iofile:\n",
    "    dataset = json.load(iofile)\n",
    "feature_matrix = np.array(dataset[\"data\"])\n",
    "targets = np.array(dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbBc_5FhGE2B"
   },
   "source": [
    "## Имплементация функций потерь и методов регуляризации.\n",
    "Для того, чтобы решить задание, вам необходимо реализовать все методы в файле `loss_and_derivatives.py`.\n",
    "__Внимание, в данном задании не требуется использовать свободный член (bias term)__, т.е. линейная модель примет простой вид\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = XW\n",
    "$$\n",
    "Единичный столбец также не добавляется к матрице $X$.\n",
    "\n",
    "Реализуйте методы для MSE, MAE, L1 и L2 регуляризации, а также вычисления их производных (опциональное задание) по параметрам линейной модели.\n",
    "\n",
    "__Для вашего удобства данные уже предобработаны, и использование линейной модели без свободного члена не является ошибкой. В данном задании он не должен быть использован.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtELlRTOGE2E",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\malne\\AppData\\Local\\Temp\\ipykernel_1112\\4175072765.py:37: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n",
      "C:\\Users\\malne\\AppData\\Local\\Temp\\ipykernel_1112\\4175072765.py:51: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "class LossAndDerivatives:\n",
    "    @staticmethod\n",
    "    def mse(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : float\n",
    "            single number with MSE value of linear model (X.dot(w)) with no bias term\n",
    "            on the selected dataset.\n",
    "\n",
    "        Comment: If Y is two-dimentional, average the error over both dimentions.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean((X.dot(w) - Y) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return: float\n",
    "            single number with MAE value of linear model (X.dot(w)) with no bias term\n",
    "            on the selected dataset.\n",
    "\n",
    "        Comment: If Y is two-dimentional, average the error over both dimentions.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return np.sum(np.abs(X.dot(w) - Y))\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_reg(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return: float\n",
    "            single number with sum of squared elements of the weight matrix ( \\sum_{ij} w_{ij}^2 )\n",
    "\n",
    "        Computes the L2 regularization term for the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return np.einsum('ij,ij->ij', w, w)\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_reg(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`)\n",
    "\n",
    "        Return : float\n",
    "            single number with sum of the absolute values of the weight matrix ( \\sum_{ij} |w_{ij}| )\n",
    "\n",
    "        Computes the L1 regularization term for the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return np.einsum('ij,ij->ij', abs(w), abs(w))\n",
    "\n",
    "    @staticmethod\n",
    "    def no_reg(w):\n",
    "        \"\"\"\n",
    "        Simply ignores the regularization\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def mse_derivative(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the MSE derivative for linear regression (X.dot(w)) with no bias term\n",
    "        w.r.t. w weight matrix.\n",
    "\n",
    "        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n",
    "        dimension as well, so you need to consider that fact in derivative implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return 2 / X.shape[1] * np.dot(X.T, (np.dot(X, w) - Y)) \n",
    "\n",
    "    @staticmethod\n",
    "    def mae_derivative(X, Y, w):\n",
    "        \"\"\"\n",
    "        X : numpy array of shape (`n_observations`, `n_features`)\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the MAE derivative for linear regression (X.dot(w)) with no bias term\n",
    "        w.r.t. w weight matrix.\n",
    "\n",
    "        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n",
    "        dimension as well, so you need to consider that fact in derivative implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        if len(Y.shape) > 1:    \n",
    "            return  X.T.dot(np.sign(X.dot(w) - Y)) / (Y.shape[0] * Y.shape[1])\n",
    "        else: return  X.T.dot(np.sign(X.dot(w) - Y)) / (Y.shape[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the L2 regularization term derivative w.r.t. the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return np.zeros_like(w)\n",
    "\n",
    "    @staticmethod\n",
    "    def l1_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n",
    "        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n",
    "\n",
    "        Return : numpy array of same shape as `w`\n",
    "\n",
    "        Computes the L1 regularization term derivative w.r.t. the weight matrix w.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        return np.zeros_like(w)\n",
    "\n",
    "    @staticmethod\n",
    "    def no_reg_derivative(w):\n",
    "        \"\"\"\n",
    "        Simply ignores the derivative\n",
    "        \"\"\"\n",
    "        return np.zeros_like(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2,  6,  8],\n",
       "       [-4, 12, 16],\n",
       "       [-3,  9, 12]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 4, 3])\n",
    "z = np.array([-1, 3, 4])\n",
    "np.einsum('i,j->ij', x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем ваше внимание, требуется реализовать решение в векторном виде (т.е. для каждого объекта предсказание $\\hat{\\mathbf{y}}$ является вектором с размерностью $\\geq 1$. При подсчете ошибки она усредняется как по объектам, так и по размерности __y__.\n",
    "\n",
    "Например, для вектора ошибок на одном объекте $[1., 1., 1., 1.]$ значение функции ошибки будет равно $\\frac{1}{4}(1. + 1. + 1. + 1.)$ \n",
    "\n",
    "Для вашего удобства метод `.mse` уже реализован и вы можете обращаться к нему за примером."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMN81aYyGE2T"
   },
   "source": [
    "Для проверки своего кода вам доступно несколько assert'ов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKUYnPWuGE2V"
   },
   "outputs": [],
   "source": [
    "w = np.array([1.0, 1.0])\n",
    "x_n, y_n = feature_matrix, targets\n",
    "\n",
    "# Repeating data to make everything multi-dimentional\n",
    "w = np.vstack([w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]).T\n",
    "y_n = np.hstack([y_n[:, None], 2 * y_n[:, None], 3 * y_n[:, None], 4 * y_n[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1344,
     "status": "error",
     "timestamp": 1582397124081,
     "user": {
      "displayName": "Victor Yacovlev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDahDnBQR6_kQQX4xt7llKTI0xt2Z802bvVR4MrqA=s64",
      "userId": "11689260236152306260"
     },
     "user_tz": -180
    },
    "id": "UtkO4hWYGE2c",
    "outputId": "cb0b99a8-2db4-4873-dfd8-741b52db29f3"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Something wrong with MSE derivative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m reference_mse_derivative \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m      2\u001b[0m     [\n\u001b[0;32m      3\u001b[0m         [\u001b[38;5;241m7.32890068\u001b[39m, \u001b[38;5;241m12.88731311\u001b[39m, \u001b[38;5;241m18.82128365\u001b[39m, \u001b[38;5;241m23.97731238\u001b[39m],\n\u001b[0;32m      4\u001b[0m         [\u001b[38;5;241m9.55674399\u001b[39m, \u001b[38;5;241m17.05397661\u001b[39m, \u001b[38;5;241m24.98807528\u001b[39m, \u001b[38;5;241m32.01723714\u001b[39m],\n\u001b[0;32m      5\u001b[0m     ]\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m reference_l2_reg_derivative \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2.54\u001b[39m, \u001b[38;5;241m2.44\u001b[39m, \u001b[38;5;241m2.9\u001b[39m, \u001b[38;5;241m2.2\u001b[39m], [\u001b[38;5;241m2.54\u001b[39m, \u001b[38;5;241m2.44\u001b[39m, \u001b[38;5;241m2.9\u001b[39m, \u001b[38;5;241m2.2\u001b[39m]])\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[0;32m     10\u001b[0m     reference_mse_derivative, LossAndDerivatives\u001b[38;5;241m.\u001b[39mmse_derivative(x_n, y_n, w), rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m     11\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething wrong with MSE derivative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[0;32m     14\u001b[0m     reference_l2_reg_derivative, LossAndDerivatives\u001b[38;5;241m.\u001b[39ml2_reg_derivative(w), rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m     15\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething wrong with L2 reg derivative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE derivative:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mL2 reg derivative:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     19\u001b[0m         LossAndDerivatives\u001b[38;5;241m.\u001b[39mmse_derivative(x_n, y_n, w),\n\u001b[0;32m     20\u001b[0m         LossAndDerivatives\u001b[38;5;241m.\u001b[39ml2_reg_derivative(w),\n\u001b[0;32m     21\u001b[0m     )\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Something wrong with MSE derivative"
     ]
    }
   ],
   "source": [
    "reference_mse_derivative = np.array(\n",
    "    [\n",
    "        [7.32890068, 12.88731311, 18.82128365, 23.97731238],\n",
    "        [9.55674399, 17.05397661, 24.98807528, 32.01723714],\n",
    "    ]\n",
    ")\n",
    "reference_l2_reg_derivative = np.array([[2.54, 2.44, 2.9, 2.2], [2.54, 2.44, 2.9, 2.2]])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mse_derivative, LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), \"Something wrong with MSE derivative\"\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l2_reg_derivative, LossAndDerivatives.l2_reg_derivative(w), rtol=1e-3\n",
    "), \"Something wrong with L2 reg derivative\"\n",
    "\n",
    "print(\n",
    "    \"MSE derivative:\\n{} \\n\\nL2 reg derivative:\\n{}\".format(\n",
    "        LossAndDerivatives.mse_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l2_reg_derivative(w),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Something wrong with L1 reg derivative",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m reference_l1_reg_derivative \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m], [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[0;32m     10\u001b[0m     reference_mae_derivative, LossAndDerivatives\u001b[38;5;241m.\u001b[39mmae_derivative(x_n, y_n, w), rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m     11\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething wrong with MAE derivative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[0;32m     14\u001b[0m     reference_l1_reg_derivative, LossAndDerivatives\u001b[38;5;241m.\u001b[39ml1_reg_derivative(w), rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m     15\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething wrong with L1 reg derivative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE derivative:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mL1 reg derivative:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     19\u001b[0m         LossAndDerivatives\u001b[38;5;241m.\u001b[39mmae_derivative(x_n, y_n, w),\n\u001b[0;32m     20\u001b[0m         LossAndDerivatives\u001b[38;5;241m.\u001b[39ml1_reg_derivative(w),\n\u001b[0;32m     21\u001b[0m     )\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Something wrong with L1 reg derivative"
     ]
    }
   ],
   "source": [
    "reference_mae_derivative = np.array(\n",
    "    [\n",
    "        [0.19708867, 0.19621798, 0.19621798, 0.19572906],\n",
    "        [0.25574138, 0.25524507, 0.25524507, 0.25406404],\n",
    "    ]\n",
    ")\n",
    "reference_l1_reg_derivative = np.array([[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mae_derivative, LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), \"Something wrong with MAE derivative\"\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l1_reg_derivative, LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3\n",
    "), \"Something wrong with L1 reg derivative\"\n",
    "\n",
    "print(\n",
    "    \"MAE derivative:\\n{} \\n\\nL1 reg derivative:\\n{}\".format(\n",
    "        LossAndDerivatives.mae_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l1_reg_derivative(w),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJcSPj8UGE20"
   },
   "source": [
    "### Градиентный спуск для решения реальной задачи\n",
    "Следующая функция позволяет найти оптимальные значения параметров с помощью градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On6aSWuIGE21"
   },
   "outputs": [],
   "source": [
    "def get_w_by_grad(X, Y, w_0, loss_mode=\"mse\", reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05):\n",
    "    if loss_mode == \"mse\":\n",
    "        loss_function = LossAndDerivatives.mse\n",
    "        loss_derivative = LossAndDerivatives.mse_derivative\n",
    "    elif loss_mode == \"mae\":\n",
    "        loss_function = LossAndDerivatives.mae\n",
    "        loss_derivative = LossAndDerivatives.mae_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Unknown loss function. Available loss functions: `mse`, `mae`\")\n",
    "\n",
    "    if reg_mode is None:\n",
    "        reg_function = LossAndDerivatives.no_reg\n",
    "        reg_derivative = LossAndDerivatives.no_reg_derivative  # lambda w: np.zeros_like(w)\n",
    "    elif reg_mode == \"l2\":\n",
    "        reg_function = LossAndDerivatives.l2_reg\n",
    "        reg_derivative = LossAndDerivatives.l2_reg_derivative\n",
    "    elif reg_mode == \"l1\":\n",
    "        reg_function = LossAndDerivatives.l1_reg\n",
    "        reg_derivative = LossAndDerivatives.l1_reg_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Unknown regularization mode. Available modes: `l1`, `l2`, None\")\n",
    "\n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n",
    "        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm > 5.0:\n",
    "            gradient = gradient / gradient_norm * 5.0\n",
    "        w -= lr * gradient\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            print(\"Step={}, loss={},\\ngradient values={}\\n\".format(i, empirical_risk, gradient))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простой пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1pyDIyqGE25"
   },
   "outputs": [],
   "source": [
    "# Initial weight matrix\n",
    "w = np.ones((2, 1), dtype=float)\n",
    "y_n = targets[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erTRQiAFGE29"
   },
   "outputs": [],
   "source": [
    "w_grad = get_w_by_grad(x_n, y_n, w, loss_mode=\"mse\", reg_mode=\"l2\", n_steps=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с `sklearn`\n",
    "Сравним реализованную модель с версией из `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=0.05)\n",
    "lr.fit(x_n, y_n)\n",
    "print(\n",
    "    \"sklearn linear regression implementation delivers MSE = {}\".format(\n",
    "        np.mean((lr.predict(x_n) - y_n) ** 2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gse1m4nyGE3C"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_n[:, -1], y_n[:, -1])\n",
    "plt.scatter(\n",
    "    x_n[:, -1],\n",
    "    x_n.dot(w_grad)[:, -1],\n",
    "    color=\"orange\",\n",
    "    label=\"Handwritten linear regression\",\n",
    "    linewidth=5,\n",
    ")\n",
    "plt.scatter(x_n[:, -1], lr.predict(x_n), color=\"cyan\", label=\"sklearn Ridge\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в полученных решениях есть небольшие различия, это не страшно. Модель основанная на вашей реализации не использует свободный член (он равен $0$), в то время версия из `sklearn` настраивает и его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GgeWdBmGE3H"
   },
   "source": [
    "### Сдача задания\n",
    "Сдайте в чекер реализованный класс `LossAndDerivatives`. Для этого можете скопировать всю ячейку с кодом (в том числе и импортирование `numpy`) в файл `derivatives.py`.\n",
    "\n",
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment0_02_linear_regression_and_gradient_descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
